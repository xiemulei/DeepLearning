# 自动微分 (Autodiff)

Burn 的张量也支持自动微分，这是任何深度学习框架的重要组成部分。我们在[上一节](./backend.html)中介绍了 `Backend` trait，但 Burn 还有另一个用于自动微分的 trait：`AutodiffBackend`。

然而，并非所有张量都支持自动微分；您需要一个同时实现 `Backend` 和 `AutodiffBackend` trait 的后端。幸运的是，您可以使用后端装饰器为任何后端添加自动微分功能：`type MyAutodiffBackend = Autodiff<MyBackend>`。这个装饰器通过维护动态计算图并利用内部后端执行张量操作，同时实现了 `AutodiffBackend` 和 `Backend` trait。

`AutodiffBackend` trait 为浮点张量添加了新的操作，这些操作在其他情况下无法调用。它还提供了一个新的关联类型 `B::Gradients`，每个计算的梯度都驻留在其中。

```rust
fn calculate_gradients<B: AutodiffBackend>(tensor: Tensor<B, 2>) -> B::Gradients {
    let mut gradients = tensor.clone().backward();

    let tensor_grad = tensor.grad(&gradients);        // 获取
    let tensor_grad = tensor.grad_remove(&mut gradients); // 弹出

    gradients
}
```

请注意，即使后端没有实现 `AutodiffBackend` trait，某些函数也始终可用。在这种情况下，这些函数将不执行任何操作。

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.detach()` | `tensor.detach()` |
| `tensor.require_grad()` | `tensor.requires_grad()` |
| `tensor.is_require_grad()` | `tensor.requires_grad` |
| `tensor.set_require_grad(require_grad)` | `tensor.requires_grad(False)` |

但是，您不太可能犯任何错误，因为您无法在不实现 `AutodiffBackend` trait 的后端上调用 `backward`。此外，如果没有自动微分后端，您无法检索张量的梯度。

## 与 PyTorch 的差异

Burn 处理梯度的方式与 PyTorch 不同。首先，当调用 `backward` 时，每个参数的 `grad` 字段不会更新。相反，反向传播返回一个容器中所有计算的梯度。这种方法提供了许多好处，例如能够轻松地将梯度发送到其他线程。

您还可以使用张量上的 `grad` 方法检索特定参数的梯度。由于此方法将梯度作为输入，因此很难忘记事先调用 `backward`。请注意，有时使用 `grad_remove` 可以通过允许就地操作来提高性能。

在 PyTorch 中，当您不需要推理或验证的梯度时，通常需要使用块来限定代码范围。

```python
# 推理模式
torch.inference():
   # 您的代码
   ...

# 或无梯度
torch.no_grad():
   # 您的代码
   ...
```

使用 Burn，您不需要用 `Autodiff` 包装后端进行推理，并且可以调用 `inner()` 来获取内部张量，这对于验证很有用。

```rust
/// 使用 `B: AutodiffBackend`
fn example_validation<B: AutodiffBackend>(tensor: Tensor<B, 2>) {
    let inner_tensor: Tensor<B::InnerBackend, 2> = tensor.inner();
    let _ = inner_tensor + 5;
}

/// 使用 `B: Backend`
fn example_inference<B: Backend>(tensor: Tensor<B, 2>) {
    let _ = tensor + 5;
    ...
}
```

### 优化器与梯度

我们已经了解了梯度如何与张量一起使用，但在使用 `burn-core` 中的优化器时，过程略有不同。要使用 `Module` trait，需要一个转换步骤来将张量参数与其梯度链接。这一步骤对于轻松支持梯度累积和多设备训练是必要的，因为每个模块可以被分叉并在不同设备上并行运行。我们将在[模块](./module.html)部分更深入地探讨这个主题。

---

*本文档翻译自 Burn 官方文档：https://burn.dev/books/burn/building-blocks/autodiff.html*
