# 张量 (Tensor)

正如之前在[模型部分](../basic-workflow/model.html)中解释的，Tensor 结构体有 3 个泛型参数：后端 B、维度 D 和数据类型。

```rust
Tensor<B, D>           // 浮点张量（默认）
Tensor<B, D, Float>    // 显式浮点张量
Tensor<B, D, Int>      // 整数张量
Tensor<B, D, Bool>     // 布尔张量
```

请注意，用于 `Float`、`Int` 和 `Bool` 张量的具体元素类型由后端实现定义。

Burn 张量由声明中的维度数 D 定义，而不是由其形状定义。张量的实际形状从其初始化中推断。例如，大小为 (5,) 的张量初始化如下：

```rust
let floats = [1.0, 2.0, 3.0, 4.0, 5.0];

// 获取默认设备
let device = Default::default();

// 正确：张量是 1 维的，包含 5 个元素
let tensor_1 = Tensor::<Backend, 1>::from_floats(floats, &device);

// 错误：let tensor_1 = Tensor::<Backend, 5>::from_floats(floats, &device);
// 这将导致错误，这是用于创建 5 维张量的
```

## 初始化

Burn 张量主要使用 `from_data()` 方法初始化，该方法接受 `TensorData` 结构体作为输入。`TensorData` 结构体有两个公共字段：`shape` 和 `dtype`。`value` 现在存储为字节，是私有的，但可以通过以下任何方法访问：`as_slice`、`as_mut_slice`、`to_vec` 和 `iter`。要从张量中检索数据，当打算后续重用张量时，应该使用 `.to_data()` 方法。或者，推荐对一次性使用使用 `.into_data()`。让我们看几个从不同输入初始化张量的例子。

```rust
// 从给定的后端（Wgpu）初始化
let tensor_1 = Tensor::<Wgpu, 1>::from_data([1.0, 2.0, 3.0], &device);

// 从通用后端初始化
let tensor_2 = Tensor::<Backend, 1>::from_data(TensorData::from([1.0, 2.0, 3.0]), &device);

// 使用 from_floats 初始化（推荐用于 f32 ElementType）
// 将在内部转换为 TensorData。
let tensor_3 = Tensor::<Backend, 1>::from_floats([1.0, 2.0, 3.0], &device);

// 从数组切片初始化整数张量
let arr: [i32; 6] = [1, 2, 3, 4, 5, 6];
let tensor_4 = Tensor::<Backend, 1, Int>::from_data(TensorData::from(&arr[0..3]), &device);

// 从自定义类型初始化

struct BodyMetrics {
    age: i8,
    height: i16,
    weight: f32
}

let bmi = BodyMetrics{
        age: 25,
        height: 180,
        weight: 80.0
    };
let data  = TensorData::from([bmi.age as f32, bmi.height as f32, bmi.weight]);
let tensor_5 = Tensor::<Backend, 1>::from_data(data, &device);
```

## 所有权和克隆

几乎所有 Burn 操作都获取输入张量的所有权。因此，多次重用张量将需要克隆它。让我们看一个例子来更好地理解所有权规则和克隆。假设我们想要对输入张量进行简单的最小-最大归一化。

```rust
let input = Tensor::<Wgpu, 1>::from_floats([1.0, 2.0, 3.0, 4.0], &device);
let min = input.min();
let max = input.max();
let input = (input - min).div(max - min);
```

对于 PyTorch 张量，上面的代码会按预期工作。然而，Rust 的严格所有权规则意味着我们需要更小心地处理所有权。

## 创建操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `Tensor::zeros(shape, device)` | `torch.zeros(shape, device=device)` |
| `Tensor::ones(shape, device)` | `torch.ones(shape, device=device)` |
| `Tensor::full(shape, value, device)` | `torch.full(shape, value, device=device)` |
| `Tensor::random(shape, distribution, device)` | N/A |
| `Tensor::random_like(distribution)` | `torch.rand_like()` 仅均匀分布 |
| `Tensor::from_data(data, device)` | `torch.tensor(data, device=device)` |
| `Tensor::from_floats(floats, device)` | N/A |
| `tensor.ones_like()` | `torch.ones_like(tensor)` |
| `tensor.zeros_like()` | `torch.zeros_like(tensor)` |
| `tensor.full_like(value)` | `torch.full_like(tensor, value)` |
| `tensor.random_like(distribution)` | `torch.rand_like(tensor)` |

## 基本操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.add(other)` | `tensor + other` |
| `tensor.add_scalar(scalar)` | `tensor + scalar` |
| `tensor.sub(other)` | `tensor - other` |
| `tensor.sub_scalar(scalar)` | `tensor - scalar` |
| `tensor.mul(other)` | `tensor * other` |
| `tensor.mul_scalar(scalar)` | `tensor * scalar` |
| `tensor.div(other)` | `tensor / other` |
| `tensor.div_scalar(scalar)` | `tensor / scalar` |
| `tensor.neg()` | `-tensor` |
| `tensor.powf(exponent)` | `tensor.pow(exponent)` |
| `tensor.powi(exponent)` | `tensor.pow(exponent)` |
| `tensor.rem(other)` | `tensor % other` |
| `tensor.rem_scalar(scalar)` | `tensor % scalar` |
| `tensor.matmul(other)` | `tensor.matmul(other)` |
| `tensor.abs()` | `tensor.abs()` |
| `tensor.acos()` | `tensor.acos()` |
| `tensor.asin()` | `tensor.asin()` |
| `tensor.atan()` | `tensor.atan()` |
| `tensor.atan2(other)` | `torch.atan2(tensor, other)` |
| `tensor.ceil()` | `tensor.ceil()` |
| `tensor.cos()` | `tensor.cos()` |
| `tensor.cosh()` | `tensor.cosh()` |
| `tensor.erf()` | `tensor.erf()` |
| `tensor.exp()` | `tensor.exp()` |
| `tensor.floor()` | `tensor.floor()` |
| `tensor.log()` | `tensor.log()` |
| `tensor.log1p()` | `tensor.log1p()` |
| `tensor.log2()` | `tensor.log2()` |
| `tensor.log10()` | `tensor.log10()` |
| `tensor.recip()` 或 `1.0 / tensor` | `tensor.reciprocal()` 或 `1.0 / tensor` |
| `tensor.round()` | `tensor.round()` |
| `tensor.sin()` | `tensor.sin()` |
| `tensor.sinh()` | `tensor.sinh()` |
| `tensor.square()` | `tensor.square()` |
| `tensor.sqrt()` | `tensor.sqrt()` |
| `tensor.tan()` | `tensor.tan()` |
| `tensor.tanh()` | `tensor.tanh()` |
| `tensor.to_full_precision()` | `tensor.to(torch.float)` |
| `tensor.trunc()` | `tensor.trunc()` |
| `tensor.var(dim)` | `tensor.var(dim)` |
| `tensor.var_bias(dim)` | N/A |
| `tensor.var_mean(dim)` | N/A |
| `tensor.var_mean_bias(dim)` | N/A |

## 形状操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.clone()` | `tensor.clone()` |
| `tensor.flatten(start, end)` | `tensor.flatten(start, end)` |
| `tensor.squeeze(dim)` | `tensor.squeeze(dim)` |
| `tensor.unsqueeze(dim)` | `tensor.unsqueeze(dim)` |
| `tensor.reshape(shape)` | `tensor.reshape(shape)` |
| `tensor.swap_dims(dim1, dim2)` | `tensor.transpose(dim1, dim2)` |
| `tensor.chunks(chunks, dim)` | `tensor.chunk(chunks, dim)` |
| `tensor.slice([start..end])` | `tensor[:, start:end]` |
| `tensor.slice_assign([start..end], values)` | `tensor[:, start:end] = values` |
| `tensor.device()` | `tensor.device` |
| `tensor.to_device(device)` | `tensor.to(device)` |
| `tensor.dims()` | `tensor.shape` |
| `tensor.to_data()` | `tensor.detach().cpu().numpy()` |

## 聚合操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.mean(dim)` | `tensor.mean(dim)` |
| `tensor.sum(dim)` | `tensor.sum(dim)` |
| `tensor.prod(dim)` | `tensor.prod(dim)` |
| `tensor.argmax(dim)` | `tensor.argmax(dim)` |
| `tensor.argmin(dim)` | `tensor.argmin(dim)` |
| `tensor.max(dim)` | `tensor.max(dim)` |
| `tensor.min(dim)` | `tensor.min(dim)` |
| `tensor.max_dim(dim)` | `tensor.max(dim)` |
| `tensor.min_dim(dim)` | `tensor.min(dim)` |

## 比较操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.equal(other)` | `tensor == other` |
| `tensor.equal_elem(scalar)` | `tensor == scalar` |
| `tensor greater(other)` | `tensor > other` |
| `tensor_greater_elem(scalar)` | `tensor > scalar` |
| `tensor greater_equal(other)` | `tensor >= other` |
| `tensor_greater_equal_elem(scalar)` | `tensor >= scalar` |
| `tensor less(other)` | `tensor < other` |
| `tensor less_elem(scalar)` | `tensor < scalar` |
| `tensor less_equal(other)` | `tensor <= other` |
| `tensor less_equal_elem(scalar)` | `tensor <= scalar` |
| `tensor.not_equal(other)` | `tensor != other` |
| `tensor.not_equal_elem(scalar)` | `tensor != scalar` |
| `tensor.argsort(descending)` | `tensor.argsort(descending)` |
| `tensor.sort(descending)` | `tensor.sort(descending)` |

## 机器学习操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.conv1d(config)` | `F.conv1d(tensor)` |
| `tensor.conv2d(config)` | `F.conv2d(tensor)` |
| `tensor.conv3d(config)` | `F.conv3d(tensor)` |
| `tensor.conv_transpose1d(config)` | `F.conv_transpose1d(tensor)` |
| `tensor.conv_transpose2d(config)` | `F.conv_transpose2d(tensor)` |
| `tensor.conv_transpose3d(config)` | `F.conv_transpose3d(tensor)` |
| `tensor.max_pool1d(config)` | `F.max_pool1d(tensor)` |
| `tensor.max_pool2d(config)` | `F.max_pool2d(tensor)` |
| `tensor.max_pool3d(config)` | `F.max_pool3d(tensor)` |
| `tensor.avg_pool1d(config)` | `F.avg_pool1d(tensor)` |
| `tensor.avg_pool2d(config)` | `F.avg_pool2d(tensor)` |
| `tensor.avg_pool3d(config)` | `F.avg_pool3d(tensor)` |
| `tensor adaptive_avg_pool1d(output_size)` | `F.adaptive_avg_pool1d(tensor)` |
| `tensor adaptive_avg_pool2d(output_size)` | `F.adaptive_avg_pool2d(tensor)` |
| `tensor adaptive_avg_pool3d(output_size)` | `F.adaptive_avg_pool3d(tensor)` |

## 选择操作

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.index_select(indices, dim)` | `torch.index_select(tensor, dim, indices)` |
| `tensor.index_assign(indices, dim, values)` | N/A |
| `tensor.gather(indices, dim)` | `torch.gather(tensor, dim, indices)` |
| `tensor.scatter(indices, dim, values)` | `torch.scatter(tensor, dim, indices, values)` |
| `tensor.mask_fill(mask, value)` | `torch.masked_fill(tensor, mask, value)` |
| `tensor.mask_where(mask, value)` | `torch.where(mask, value, tensor)` |

## 整数操作

这些操作仅适用于 `Int` 张量。

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `Tensor::arange(5..10, device)` | `tensor.arange(start=5, end=10, device=device)` |
| `Tensor::arange_step(5..10, 2, device)` | `tensor.arange(start=5, end=10, step=2, device=device)` |
| `tensor.bitwise_and(other)` | `torch.bitwise_and(tensor, other)` |
| `tensor.bitwise_and_scalar(scalar)` | `torch.bitwise_and(tensor, scalar)` |
| `tensor.bitwise_not()` | `torch.bitwise_not(tensor)` |
| `tensor.bitwise_left_shift(other)` | `torch.bitwise_left_shift(tensor, other)` |
| `tensor.bitwise_left_shift_scalar(scalar)` | `torch.bitwise_left_shift(tensor, scalar)` |
| `tensor.bitwise_right_shift(other)` | `torch.bitwise_right_shift(tensor, other)` |
| `tensor.bitwise_right_shift_scalar(scalar)` | `torch.bitwise_right_shift(tensor, scalar)` |
| `tensor.bitwise_or(other)` | `torch.bitwise_or(tensor, other)` |
| `tensor.bitwise_or_scalar(scalar)` | `torch.bitwise_or(tensor, scalar)` |
| `tensor.bitwise_xor(other)` | `torch.bitwise_xor(tensor, other)` |
| `tensor.bitwise_xor_scalar(scalar)` | `torch.bitwise_xor(tensor, scalar)` |
| `tensor.float()` | `tensor.to(torch.float)` |
| `tensor.from_ints(ints)` | N/A |
| `tensor.int_random(shape, distribution, device)` | N/A |
| `tensor.cartesian_grid(shape, device)` | N/A |

## 布尔操作

这些操作仅适用于 `Bool` 张量。

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `Tensor::diag_mask(shape, diagonal)` | N/A |
| `Tensor::tril_mask(shape, diagonal)` | N/A |
| `Tensor::triu_mask(shape, diagonal)` | N/A |
| `tensor.argwhere()` | `tensor.argwhere()` |
| `tensor.bool_and()` | `tensor.logical_and()` |
| `tensor.bool_not()` | `tensor.logical_not()` |
| `tensor.bool_or()` | `tensor.logical_or()` |
| `tensor.bool_xor()` | `tensor.logical_xor()` |
| `tensor.float()` | `tensor.to(torch.float)` |
| `tensor.int()` | `tensor.to(torch.long)` |
| `tensor.nonzero()` | `tensor.nonzero(as_tuple=True)` |

## 量化操作

这些操作仅适用于实现量化策略的后端上的 `Float` 张量。

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `tensor.quantize(scheme, qparams)` | N/A |
| `tensor.dequantize()` | N/A |

## 激活函数

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `activation::gelu(tensor)` | `nn.functional.gelu(tensor)` |
| `activation::hard_sigmoid(tensor, alpha, beta)` | `nn.functional.hardsigmoid(tensor)` |
| `activation::leaky_relu(tensor, negative_slope)` | `nn.functional.leaky_relu(tensor, negative_slope)` |
| `activation::log_sigmoid(tensor)` | `nn.functional.log_sigmoid(tensor)` |
| `activation::log_softmax(tensor, dim)` | `nn.functional.log_softmax(tensor, dim)` |
| `activation::mish(tensor)` | `nn.functional.mish(tensor)` |
| `activation::prelu(tensor,alpha)` | `nn.functional.prelu(tensor,weight)` |
| `activation::quiet_softmax(tensor, dim)` | `nn.functional.quiet_softmax(tensor, dim)` |
| `activation::relu(tensor)` | `nn.functional.relu(tensor)` |
| `activation::sigmoid(tensor)` | `nn.functional.sigmoid(tensor)` |
| `activation::silu(tensor)` | `nn.functional.silu(tensor)` |
| `activation::softmax(tensor, dim)` | `nn.functional.softmax(tensor, dim)` |
| `activation::softmin(tensor, dim)` | `nn.functional.softmin(tensor, dim)` |
| `activation::softplus(tensor, beta)` | `nn.functional.softplus(tensor, beta)` |
| `activation::tanh(tensor)` | `nn.functional.tanh(tensor)` |

## 网格函数

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `grid::meshgrid(tensors, GridIndexing::Matrix)` | `torch.meshgrid(tensors, indexing="ij")` |
| `grid::meshgrid(tensors, GridIndexing::Cartesian)` | `torch.meshgrid(tensors, indexing="xy")` |

## 线性代数函数

| Burn API | PyTorch 等价物 |
|---------|---------------|
| `linalg::vector_norm(tensors, p, dim)` | `torch.linalg.vector_norm(tensor, p, dim)` |
| `linalg::diag(tensor)` | `torch.diag(tensor)` |
| `linalg::trace(tensor)` | `torch.trace(tensor)` |
| `linalg::outer(x, y)` | `torch.outer(x, y)` / `einsum("bi,bj->bij", …)` |
| `linalg::lu_decomposition(tensor)` | `torch.linalg.lu(tensor)` |

## 显示张量详情

Burn 提供了灵活的选项来显示张量信息，允许您控制详细程度和格式以满足您的需求。

### 基本显示

要显示张量的详细视图，您可以简单地使用 Rust 的 `println!` 或 `format!` 宏：

```rust
let tensor = Tensor::<Backend, 2>::full([2, 3], 0.123456789, &Default::default());
println!("{}", tensor);
```

这将输出：

```
Tensor {
  data:
[[0.12345679, 0.12345679, 0.12345679],
 [0.12345679, 0.12345679, 0.12345679]],
  shape:  [2, 3],
  device:  Cpu,
  backend:  "ndarray",
  kind:  "Float",
  dtype:  "f32",
}
```

### 控制精度

您可以使用 Rust 的格式化语法控制显示的小数位数：

```rust
println!("{:.2}", tensor);
```

输出：

```
Tensor {
  data:
[[0.12, 0.12, 0.12],
 [0.12, 0.12, 0.12]],
  shape:  [2, 3],
  device:  Cpu,
  backend:  "ndarray",
  kind:  "Float",
  dtype:  "f32",
}
```

### 全局打印选项

对于更精细的张量打印控制，Burn 提供了 `PrintOptions` 结构体和 `set_print_options` 函数：

```rust
use burn::tensor::{set_print_options, PrintOptions};

let print_options = PrintOptions {
    precision: Some(2),
    ..Default::default()
};

set_print_options(print_options);
```

选项：
- `precision`：浮点数的小数位数（默认：None）
- `threshold`：开始摘要前要显示的最大元素数（默认：1000）
- `edge_items`：摘要时在每个维度开始和结束显示的项目数（默认：3）

### 检查张量相似性

Burn 提供了一个实用函数 `check_closeness` 来比较两个张量并评估它们的相似性。这个函数在调试和验证张量操作时特别有用，特别是在处理浮点运算时，小的数值差异可能会累积。在从其他框架导入模型时比较模型输出时也很有价值，有助于确保导入的模型产生与原始一致的结果。

以下是使用 `check_closeness` 的示例：

```rust
use burn::tensor::{check_closeness, Tensor};
type B = burn::backend::NdArray;

let device = Default::default();
let tensor1 = Tensor::<B, 1>::from_floats(
    [1.0, 2.0, 3.0, 4.0, 5.0, 6.001, 7.002, 8.003, 9.004, 10.1],
    &device,
);
let tensor2 = Tensor::<B, 1>::from_floats(
    [1.0, 2.0, 3.0, 4.000, 5.0, 6.0, 7.001, 8.002, 9.003, 10.004],
    &device,
);

check_closeness(&tensor1, &tensor2);
```

`check_closeness` 函数逐元素比较两个输入张量，检查它们的绝对差异与一系列 epsilon 值的关系。然后它打印一个详细报告，显示在每个容差级别内的元素百分比。

输出为不同的 epsilon 值提供分解，允许您评估各种精度级别上张量的接近程度。这在处理可能引入小数值差异的操作时特别有帮助。

该函数使用彩色输出来突出显示结果：
- 绿色 [PASS]：所有元素都在指定的容差范围内。
- 黄色 [WARN]：大多数元素（90% 或更多）在容差范围内。
- 红色 [FAIL]：检测到显著差异。

这个实用程序在实现或调试张量操作时可能非常有价值，特别是那些涉及复杂数学计算或从其他框架移植算法的操作。在验证导入模型的准确性时，它也是一个必不可少的工具，确保 Burn 实现产生的结果与原始模型的结果密切匹配。

---

*本文档翻译自 Burn 官方文档：https://burn.dev/books/burn/building-blocks/tensor.html*
