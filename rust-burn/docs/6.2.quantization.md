# 量化 (Quantization)

量化技术以较低精度的数据类型（如 8 位整数）执行计算和存储张量，而不是浮点精度。有多种量化深度学习模型的方法，分类为：

- 训练后量化 (PTQ)
- 量化感知训练 (QAT)

在训练后量化中，模型以浮点精度训练，然后转换为较低精度的数据类型。训练后量化有两种类型：

1. **静态量化**：量化模型的权重和激活。静态量化激活需要数据校准（即，记录激活值以使用代表性数据计算最佳量化参数）。
2. **动态量化**：提前量化权重（如静态量化），但激活在运行时动态量化。

有时训练后量化无法达到可接受的任务精度。这就是量化感知训练发挥作用的地方，因为它在训练期间模拟量化的影响。量化误差因此使用伪量化模块在前向和反向传播中建模，这有助于模型学习对精度降低更具鲁棒的表示。

<div class="warning">
<p>Burn 中的量化支持目前正在积极开发中。</p>
<p>它在某些后端上支持以下模式：</p>
<ul>
<li>每张量和每块（线性）量化为 8 位、4 位和 2 位表示</li>
</ul>
<p>目前不支持整数操作，这意味着张量被反量化以浮点精度执行操作。</p>
</div>
## 模块量化 (Module Quantization)

训练后量化模型的权重非常简单。我们可以访问权重张量并收集它们的统计信息，如使用 `MinMaxCalibration` 时的最小值和最大值，以计算量化参数。

```rust
use burn::module::Quantizer;
use burn::tensor::quantization::{Calibration, QuantLevel, QuantParam, QuantScheme, QuantValue};

// 量化配置
let scheme = QuantScheme::default()
    .with_level(QuantLevel::Block(32))
    .with_value(QuantValue::Q4F)
    .with_param(QuantParam::F16);
let mut quantizer = Quantizer {
    calibration: Calibration::MinMax,
    scheme,
};

// 量化权重
let model = model.quantize_weights(&mut quantizer);
```

### 校准 (Calibration)

校准是量化过程中的步骤，其中计算所有浮点张量的范围。这对于权重来说相当简单，因为实际范围在*量化时间*是已知的（权重是静态的），但激活需要更多关注。

为了计算量化参数，Burn 支持以下 `Calibration` 方法。

<div class="table-wrapper"><table><thead><tr><th style="text-align: left">方法</th><th style="text-align: left">描述</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>MinMax</code></td><td style="text-align: left">基于运行的最小值和最大值计算量化范围映射。</td></tr>
</tbody></table>
</div>

### 量化方案 (Quantization Scheme)

量化方案定义了输入如何量化，包括量化值的表示、存储格式、粒度和值如何缩放。

```rust
let scheme = QuantScheme::default()
    .with_mode(QuantMode::Symmetric)         // 量化模式
    .with_level(QuantLevel::block([2, 16]))  // 粒度（每张量或每块）
    .with_value(QuantValue::Q8S)             // 量化值的数据类型，独立于它们的存储方式
    .with_store(QuantStore::Native)          // 量化值的存储格式
    .with_param(QuantParam::F16);            // 量化参数的精度
```

#### 量化模式 (Quantization Mode)

<div class="table-wrapper"><table><thead><tr><th style="text-align: left">模式</th><th style="text-align: left">描述</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>Symmetric</code></td><td style="text-align: left">值围绕零对称缩放。</td></tr>
</tbody></table>
</div>

#### 量化级别 (Quantization Level)

<div class="table-wrapper"><table><thead><tr><th style="text-align: left">级别</th><th style="text-align: left">描述</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>Tensor</code></td><td style="text-align: left">整个张量的单一量化参数集。</td></tr>
<tr><td style="text-align: left"><code>Block(block_size: BlockSize)</code></td><td style="text-align: left">张量被 block_size 定义的块（1D、2D 或更高）分割，每个块都有自己的量化参数。</td></tr>
</tbody></table>
</div>

#### 量化值 (Quantization Value)

<div class="table-wrapper"><table><thead><tr><th style="text-align: left">值</th><th style="text-align: center">位数</th><th style="text-align: left">描述</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>Q8F</code></td><td style="text-align: center">8</td><td style="text-align: left">8 位全范围量化</td></tr>
<tr><td style="text-align: left"><code>Q4F</code></td><td style="text-align: center">4</td><td style="text-align: left">4 位全范围量化</td></tr>
<tr><td style="text-align: left"><code>Q2F</code></td><td style="text-align: center">2</td><td style="text-align: left">2 位全范围量化</td></tr>
<tr><td style="text-align: left"><code>Q8S</code></td><td style="text-align: center">8</td><td style="text-align: left">8 位对称量化</td></tr>
<tr><td style="text-align: left"><code>Q4S</code></td><td style="text-align: center">4</td><td style="text-align: left">4 位对称量化</td></tr>
<tr><td style="text-align: left"><code>Q2S</code></td><td style="text-align: center">2</td><td style="text-align: left">2 位对称量化</td></tr>
<tr><td style="text-align: left"><code>E5M2</code></td><td style="text-align: center">8</td><td style="text-align: left">8 位浮点（5 指数，2 尾数）</td></tr>
<tr><td style="text-align: left"><code>E4M3</code></td><td style="text-align: center">8</td><td style="text-align: left">8 位浮点（4 指数，3 尾数）</td></tr>
<tr><td style="text-align: left"><code>E2M1</code></td><td style="text-align: center">4</td><td style="text-align: left">4 位浮点（2 指数，1 尾数）</td></tr>
</tbody></table>
</div>

#### 量化存储 (Quantization Store)

<div class="table-wrapper"><table><thead><tr><th style="text-align: left">存储</th><th style="text-align: left">描述</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>Native</code></td><td style="text-align: left">每个量化值直接存储在内存中。</td></tr>
<tr><td style="text-align: left"><code>32</code></td><td style="text-align: left">多个量化值打包到 32 位整数中。</td></tr>
</tbody></table>
</div>

<p>子字节量化值不支持本机存储。</p>

#### 量化参数精度 (Quantization Parameters Precision)

<div class="table-wrapper"><table><thead><tr><th style="text-align: left">参数</th><th style="text-align: left">描述</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>F32</code></td><td style="text-align: left">全浮点精度。</td></tr>
<tr><td style="text-align: left"><code>F16</code></td><td style="text-align: left">半精度浮点。</td></tr>
<tr><td style="text-align: left"><code>BF16</code></td><td style="text-align: left">脑浮点 16 位精度。</td></tr>
</tbody></table>
</div>

---

*本文档翻译自 Burn 官方文档：https://burn.dev/books/burn/performance/quantization.html*
