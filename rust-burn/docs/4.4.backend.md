# 后端 (Backend)

我们已经有效地编写了训练模型所需的大部分必要代码。然而，我们还没有在任何地方明确指定要使用的后端。这将在我们程序的主入口点中定义，即在 `src/main.rs` 中定义的 `main` 函数。

```rust
#![recursion_limit = "256"]
mod data;
mod model;
mod training;

use crate::{model::ModelConfig, training::TrainingConfig};
use burn::{
    backend::{Autodiff, Wgpu},
    data::dataset::Dataset,
    optim::AdamConfig,
};

fn main() {
    type MyBackend = Wgpu<f32, i32>;
    type MyAutodiffBackend = Autodiff<MyBackend>;

    let device = burn::backend::wgpu::WgpuDevice::default();
    let artifact_dir = "/tmp/guide";
    crate::training::train::<MyAutodiffBackend>(
        artifact_dir,
        TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),
        device.clone(),
    );
}
```

在这个代码片段中，我们使用 `Wgpu` 后端，它与任何操作系统兼容并将使用 GPU。有关其他选项，请参见 Burn README。此后端类型接受图形 API、浮点类型和整数类型作为在训练期间使用的泛型参数。自动微分后端只是相同后端，包装在 `Autodiff` 结构体中，该结构体为任何后端赋予可微性。

我们使用工件目录调用之前定义的 `train` 函数、模型配置（数字类别数为 10，隐藏维度为 512）、优化器配置，在我们的情况下将是默认 Adam 配置，以及可以从后端获取的设备。

您现在可以使用以下命令训练您新创建的模型：

```bash
cargo run --release
```

当使用上述命令运行您的项目时，您应该通过基本的 CLI 仪表板看到训练进度：

<img title="a title" alt="Alt text" src="./images/training-output.png">

---

*本文档翻译自 Burn 官方文档：https://burn.dev/books/burn/basic-workflow/backend.html*
